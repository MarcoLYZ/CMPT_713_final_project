{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ddc11b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6338/35127313.py:28: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use(\"seaborn-whitegrid\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re, string\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "seed = 8\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1cee0c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"./data/yahoo_answers_csv/test_cleaned.csv\")\n",
    "test_df = test_df[test_df['text_clean'].notnull()] \n",
    "test_df['target'] = test_df['target'] - 1\n",
    "\n",
    "train_df = pd.read_csv(\"./data/yahoo_answers_csv/train_cleaned.csv\")\n",
    "train_df = train_df[train_df['text_clean'].notnull()] \n",
    "train_df['target'] = train_df['target'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "aa00be4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df):\n",
    "    text_lens = []\n",
    "    for text in df.text_clean:\n",
    "        text_len = len(text.split())\n",
    "        text_lens.append(text_len)\n",
    "    df['text_len'] = text_lens\n",
    "    df = df[df['text_len'] > 3]\n",
    "    df = df[df['text_len'] < 50]\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_feature(column, vocab_to_int):\n",
    "    ##Tokenize the columns text using the vocabulary\n",
    "    text_int = []\n",
    "    for text in column:\n",
    "        r = [vocab_to_int[word] for word in text.split() if word in vocab_to_int.keys()]\n",
    "        text_int.append(r)\n",
    "    ##Add padding to tokens\n",
    "    features = np.zeros((len(text_int), seq_len), dtype = int)\n",
    "    for i, review in enumerate(text_int):\n",
    "        if len(review) <= seq_len:\n",
    "            zeros = list(np.zeros(seq_len - len(review)))\n",
    "            new = zeros + review\n",
    "        else:\n",
    "            new = review[: seq_len]\n",
    "        features[i, :] = np.array(new)\n",
    "    return features\n",
    "    \n",
    "    \n",
    "def Tokenize(column_train, column_test, seq_len):\n",
    "    ##Create vocabulary of words from column\n",
    "    column = column_train + column_test\n",
    "    corpus = [word for text in column for word in text.split()]\n",
    "    count_words = Counter(corpus)\n",
    "    # sorted_words = count_words.most_common()\n",
    "    # vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "    count_words_now = [(w, c) for (w,c) in count_words.items() if c < 30000 and c > 50]\n",
    "    vocab_to_int = {w:i+1 for i, (w,c) in enumerate(count_words_now)}\n",
    "\n",
    "    features_train = generate_feature(column_train, vocab_to_int)\n",
    "    features_test = generate_feature(column_test, vocab_to_int)\n",
    "\n",
    "    return vocab_to_int, features_train, features_test\n",
    "\n",
    "\n",
    "test_df = pre_process(test_df)\n",
    "train_df = pre_process(train_df)\n",
    "vocabulary_train, tokenized_column_train, tokenized_column_test = Tokenize(list(train_df[\"text_clean\"]), list(test_df[\"text_clean\"]), 69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "34aa6bac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7f793bda7d90>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word2vec_train_data = list(map(lambda x: x.split(), train_df[\"text_clean\"]))\n",
    "EMBEDDING_DIM = 200\n",
    "word2vec_model = Word2Vec(Word2vec_train_data, vector_size=EMBEDDING_DIM)\n",
    "VOCAB_SIZE = len(vocabulary_train) + 1\n",
    "word2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6a2cf54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix Shape: (16570, 200)\n"
     ]
    }
   ],
   "source": [
    "#define empty embedding matrix\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "    \n",
    "#fill the embedding matrix with the pre trained values from word2vec\n",
    "#    corresponding to word (string), token (number associated to the word)\n",
    "for word, token in vocabulary_train.items():\n",
    "    if word2vec_model.wv.__contains__(word):\n",
    "        embedding_matrix[token] = word2vec_model.wv.__getitem__(word)\n",
    "\n",
    "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "42f22823",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(tokenized_column_train), torch.from_numpy(train_df['target'].to_numpy()))\n",
    "# test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "valid_data = TensorDataset(torch.from_numpy(tokenized_column_test), torch.from_numpy(test_df['target'].to_numpy()))\n",
    "BATCH_SIZE = 1024\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True) \n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
    "# test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6c58e541",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "HIDDEN_DIM = 100 \n",
    "LSTM_LAYERS = 1\n",
    "\n",
    "LR = 3e-4 \n",
    "DROPOUT = 0.5\n",
    "BIDIRECTIONAL = True \n",
    "EPOCHS = 40 \n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b7785558",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Sentiment_Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, lstm_layers, bidirectional,batch_size, dropout):\n",
    "        super(BiLSTM_Sentiment_Classifier,self).__init__()\n",
    "        \n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            dropout=dropout,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        self.batch_size = x.size(0)\n",
    "        embedded = self.embedding(x)\n",
    "        out, hidden = self.lstm(embedded, hidden)\n",
    "        out = out[:,-1,:]\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        #Initialization of the LSTM hidden and cell states\n",
    "        h0 = torch.zeros((self.lstm_layers*self.num_directions, batch_size, self.hidden_dim)).detach().to(DEVICE)\n",
    "        c0 = torch.zeros((self.lstm_layers*self.num_directions, batch_size, self.hidden_dim)).detach().to(DEVICE)\n",
    "        hidden = (h0, c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dae8d3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM_Sentiment_Classifier(\n",
      "  (embedding): Embedding(16570, 200)\n",
      "  (lstm): LSTM(200, 100, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/anaconda3/envs/CMPT_713_final_project/lib/python3.10/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_Sentiment_Classifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM,NUM_CLASSES, LSTM_LAYERS,BIDIRECTIONAL, BATCH_SIZE, DROPOUT)\n",
    "model = model.to(DEVICE)\n",
    "model.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "model.embedding.weight.requires_grad=True\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay = 5e-6)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0982ae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Epoch: 0\n",
      "Epoch 1:Validation accuracy increased (0.000000 --> 67.460938).  Saving model ...\n",
      "\tTrain_loss : 1.1713 Val_loss : 1.0182\n",
      "\tTrain_acc : 62.472% Val_acc : 67.461%\n",
      "Start Epoch: 1\n",
      "Epoch 2:Validation accuracy increased (67.460938 --> 68.400879).  Saving model ...\n",
      "\tTrain_loss : 0.9889 Val_loss : 0.9817\n",
      "\tTrain_acc : 67.864% Val_acc : 68.401%\n",
      "Start Epoch: 2\n",
      "Epoch 3:Validation accuracy increased (68.400879 --> 68.984375).  Saving model ...\n",
      "\tTrain_loss : 0.9554 Val_loss : 0.9663\n",
      "\tTrain_acc : 68.846% Val_acc : 68.984%\n",
      "Start Epoch: 3\n",
      "Epoch 4:Validation accuracy increased (68.984375 --> 69.089355).  Saving model ...\n",
      "\tTrain_loss : 0.9327 Val_loss : 0.9560\n",
      "\tTrain_acc : 69.533% Val_acc : 69.089%\n",
      "Start Epoch: 4\n",
      "Epoch 5:Validation accuracy increased (69.089355 --> 69.333496).  Saving model ...\n",
      "\tTrain_loss : 0.9147 Val_loss : 0.9527\n",
      "\tTrain_acc : 70.062% Val_acc : 69.333%\n",
      "Start Epoch: 5\n",
      "Epoch 6:Validation accuracy increased (69.333496 --> 69.521484).  Saving model ...\n",
      "\tTrain_loss : 0.8991 Val_loss : 0.9482\n",
      "\tTrain_acc : 70.525% Val_acc : 69.521%\n",
      "Start Epoch: 6\n",
      "Epoch 7:Validation accuracy did not increase\n",
      "\tTrain_loss : 0.8855 Val_loss : 0.9471\n",
      "\tTrain_acc : 70.932% Val_acc : 69.404%\n",
      "Start Epoch: 7\n",
      "Epoch 8:Validation accuracy increased (69.521484 --> 69.526367).  Saving model ...\n",
      "\tTrain_loss : 0.8732 Val_loss : 0.9479\n",
      "\tTrain_acc : 71.286% Val_acc : 69.526%\n",
      "Start Epoch: 8\n",
      "Epoch 9:Validation accuracy did not increase\n",
      "\tTrain_loss : 0.8617 Val_loss : 0.9494\n",
      "\tTrain_acc : 71.647% Val_acc : 69.504%\n",
      "Start Epoch: 9\n",
      "Epoch 10:Validation accuracy did not increase\n",
      "\tTrain_loss : 0.8510 Val_loss : 0.9521\n",
      "\tTrain_acc : 71.963% Val_acc : 69.480%\n",
      "Start Epoch: 10\n",
      "Epoch 11:Validation accuracy did not increase\n",
      "\tTrain_loss : 0.8406 Val_loss : 0.9550\n",
      "\tTrain_acc : 72.289% Val_acc : 69.226%\n",
      "Start Epoch: 11\n",
      "Epoch 12:Validation accuracy did not increase\n",
      "\tTrain_loss : 0.8307 Val_loss : 0.9609\n",
      "\tTrain_acc : 72.584% Val_acc : 69.141%\n",
      "Start Epoch: 12\n",
      "Epoch 13:Validation accuracy did not increase\n",
      "Early stopped at epoch : 13\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "total_step_val = len(valid_loader)\n",
    "\n",
    "early_stopping_patience = 4\n",
    "early_stopping_counter = 0\n",
    "\n",
    "valid_acc_max = 0 # Initialize best accuracy top 0\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    print(\"Start Epoch: \" + str(e))\n",
    "    #lists to host the train and validation losses of every batch for each epoch\n",
    "    train_loss, valid_loss  = [], []\n",
    "    #lists to host the train and validation accuracy of every batch for each epoch\n",
    "    train_acc, valid_acc  = [], []\n",
    "\n",
    "    #lists to host the train and validation predictions of every batch for each epoch\n",
    "    y_train_list, y_val_list = [], []\n",
    "\n",
    "    #initalize number of total and correctly classified texts during training and validation\n",
    "    correct, correct_val = 0, 0\n",
    "    total, total_val = 0, 0\n",
    "    running_loss, running_loss_val = 0, 0\n",
    "\n",
    "\n",
    "    ####TRAINING LOOP####\n",
    "\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE) #load features and targets in device\n",
    "\n",
    "        h = model.init_hidden(labels.size(0))\n",
    "\n",
    "        model.zero_grad() #reset gradients \n",
    "\n",
    "        output, h = model(inputs,h) #get output and hidden states from LSTM network\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_train = torch.argmax(output, dim=1) #get tensor of predicted values on the training set\n",
    "        y_train_list.extend(y_pred_train.squeeze().tolist()) #transform tensor to list and the values to the list\n",
    "        \n",
    "        correct += torch.sum(y_pred_train==labels).item() #count correctly classified texts per batch\n",
    "        total += labels.size(0) #count total texts per batch\n",
    "\n",
    "    train_loss.append(running_loss / total_step)\n",
    "    train_acc.append(100 * correct / total)\n",
    "\n",
    "    ####VALIDATION LOOP####\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            val_h = model.init_hidden(labels.size(0))\n",
    "\n",
    "            output, val_h = model(inputs, val_h)\n",
    "\n",
    "            val_loss = criterion(output, labels)\n",
    "            running_loss_val += val_loss.item()\n",
    "\n",
    "            y_pred_val = torch.argmax(output, dim=1)\n",
    "            y_val_list.extend(y_pred_val.squeeze().tolist())\n",
    "\n",
    "            correct_val += torch.sum(y_pred_val==labels).item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "        valid_loss.append(running_loss_val / total_step_val)\n",
    "        valid_acc.append(100 * correct_val / total_val)\n",
    "\n",
    "    #Save model if validation accuracy increases\n",
    "    if np.mean(valid_acc) >= valid_acc_max:\n",
    "        torch.save(model.state_dict(), './state_dict.pt')\n",
    "        print(f'Epoch {e+1}:Validation accuracy increased ({valid_acc_max:.6f} --> {np.mean(valid_acc):.6f}).  Saving model ...')\n",
    "        valid_acc_max = np.mean(valid_acc)\n",
    "        early_stopping_counter=0 #reset counter if validation accuracy increases\n",
    "    else:\n",
    "        print(f'Epoch {e+1}:Validation accuracy did not increase')\n",
    "        early_stopping_counter+=1 #increase counter if validation accuracy does not increase\n",
    "        \n",
    "    if early_stopping_counter > early_stopping_patience:\n",
    "        print('Early stopped at epoch :', e+1)\n",
    "        break\n",
    "    \n",
    "    print(f'\\tTrain_loss : {np.mean(train_loss):.4f} Val_loss : {np.mean(valid_loss):.4f}')\n",
    "    print(f'\\tTrain_acc : {np.mean(train_acc):.3f}% Val_acc : {np.mean(valid_acc):.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./state_dict.pt'))\n",
    "model.eval()\n",
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "    test_h = model.init_hidden(labels.size(0))\n",
    "\n",
    "    output, val_h = model(inputs, test_h)\n",
    "    y_pred_test = torch.argmax(output, dim=1)\n",
    "    y_pred_list.extend(y_pred_test.squeeze().tolist())\n",
    "    y_test_list.extend(labels.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204205b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test_list,y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e5beca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
